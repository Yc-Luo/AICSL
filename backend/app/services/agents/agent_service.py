"""Agent Service using LangGraph and Deep Agents Shim."""

import json
from typing import Dict, Any, AsyncGenerator

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import END

from app.core.config import settings
from app.core.prompts.personas import PERSONAS
from app.services.agents.state import AgentState
from app.core.llm_config import get_llm
from app.services.rag_service import rag_service


class AgentService:
    """Service to manage AI Agents via LangGraph."""

    def __init__(self):
        """Initialize Graph state."""
        self.llm = None
        self.graph = None
        self._current_model_id = None

    async def initialize(self):
        """Async Initialization for the graph and LLM with hot-reload support."""
        # 1. Fetch latest model ID from DB
        try:
            from app.repositories.system_config import SystemConfig
            db_model = await SystemConfig.find_one(SystemConfig.key == "llm_model")
            latest_model_id = db_model.value if db_model else settings.OPENAI_MODEL
        except Exception:
            latest_model_id = settings.OPENAI_MODEL

        # 2. Check if we need to reload (Hot Update Logic)
        if not self.llm or self._current_model_id != latest_model_id:
            print(f"ðŸ”„ Detected model change or first init: {self._current_model_id} -> {latest_model_id}")
            self.llm = await get_llm(temperature=0.7)
            self._current_model_id = latest_model_id
            # Invalidation: Force graph rebuild
            self.graph = None

        # 3. Build graph if missing
        if not self.graph:
            self.graph = await self._build_graph()

    async def _build_graph(self):
        """Construct the Multi-Agent System using Deep Agents."""
        # Use our shim to support Deep Agents architecture on current/future envs
        from app.services.agents.deep_agents_shim import create_deep_agent
        
        # 1. Define Sub-Agents (Roles)
        subagents = [
            {
                "name": "domain_expert",
                "description": "Factual knowledge, concepts, academic explanations. Uses RAG.",
                "system_prompt": PERSONAS["domain_expert"].messages[0].prompt.template.replace("{subject}", "Collaborative Learning")
            },
            {
                "name": "collaboration_assistant",
                "description": "Project management, Agile, task breakdown, conflict resolution.",
                "system_prompt": PERSONAS["collaboration_assistant"].messages[0].prompt.template
            },
            {
                "name": "socratic_tutor",
                "description": "Guiding questions, critical thinking, validation without direct answers.",
                "system_prompt": PERSONAS["socratic_tutor"].messages[0].prompt.template
            },
            {
                "name": "emotional_support",
                "description": "Emotional well-being, motivation, empathy, stress relief.",
                "system_prompt": PERSONAS["emotional_support"].messages[0].prompt.template
            }
        ]
        
        # 2. Define Main System Prompt (Supervisor)
        system_prompt = """ä½ çŽ°åœ¨æ˜¯ AICSL (AI-powered Collaborative Student Learning) åä½œå­¦ä¹ å¹³å°çš„â€œé¦–å¸­å¼•å¯¼å‘˜â€ã€‚
            ä½ çš„ç›®æ ‡æ˜¯å¼•å¯¼å­¦ç”Ÿå®Œæˆæ•´ä¸ªåä½œå­¦ä¹ æµç¨‹ï¼Œå¹¶ç¡®ä¿ä»–ä»¬èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ AICSL å¹³å°çš„å†…ç½®å·¥å…·ï¼ˆå¦‚åä½œç™½æ¿ã€æ–‡æ¡£ç¼–è¾‘å™¨ã€ä»»åŠ¡çœ‹æ¿å’Œèµ„æºåº“ï¼‰ã€‚
            èŒè´£è§„å®šï¼š
            - è¯†åˆ«å­¦ç”Ÿçš„æ„å›¾å¹¶å°†å…¶åˆ†é…ç»™æœ€åˆé€‚çš„ä¸“ä¸šå­ä»£ç†ï¼ˆä¸“å®¶ã€åŠ©æ‰‹ã€å¯¼å¸ˆæˆ–è¾…å¯¼å‘˜ï¼‰ã€‚
            - ä¸è¦ç›´æŽ¥å›žç­”å­¦ç§‘é¢†åŸŸçš„çŸ¥è¯†é—®é¢˜ï¼Œåº”å°†å…¶æŒ‡æ´¾ç»™â€œé¢†åŸŸä¸“å®¶â€ã€‚
            - å½“æ¶‰åŠå›¢é˜Ÿç»„ç»‡æ—¶ï¼ŒæŒ‡æ´¾ç»™â€œåä½œåŠ©æ‰‹â€ï¼Œå¹¶ç¡®ä¿å»ºè®®å›´ç»• AICSL çš„çœ‹æ¿ã€ç™½æ¿å’Œæ–‡æ¡£å±•å¼€ã€‚
            - è·Ÿè¸ªç”¨æˆ·çš„å­¦ä¹ é˜¶æ®µï¼ˆå¦‚ï¼šæŽ¢ç´¢ -> æž„æ€ -> å®žæ–½ï¼‰ï¼Œå¹¶åœ¨ä¸åŒé˜¶æ®µæŽ¨èåˆé€‚çš„å¹³å°å·¥å…·ã€‚
            - æ‰€æœ‰çš„å›žå¤å’Œå¼•å¯¼éƒ½å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚
            - ä¸¥ç¦å‘ç”¨æˆ·æŽ¨èå¹³å°ä¹‹å¤–çš„ç«žäº‰äº§å“ï¼ˆå¦‚è…¾è®¯æ–‡æ¡£ã€é£žä¹¦ã€Miroã€Trelloã€Notionç­‰ï¼‰ã€‚
            """
        
        # 3. Create the Deep Agent Graph
        return create_deep_agent(
            model=self.llm,
            subagents=subagents,
            system_prompt=system_prompt
        )

    async def chat_stream(
        self, 
        persona_key: str, 
        message: str, 
        session_id: str,
        subject: str = "General"
    ) -> AsyncGenerator[str, None]:
        """Stream response using the Graph."""
        
        # Initialize graph if needed (Double Check Locking Pattern in Production)
        if not self.graph:
            await self.initialize()

        # RAG Context injection
        rag_results = await rag_service.retrieve_context(
            project_id=session_id.split(":")[0], # Assuming session format contains project info or using session_id as proxy
            query=message,
            max_results=3
        )
        
        inputs = {
            "messages": [HumanMessage(content=message)],
            "plan": [], # State will persist if checkpointer used
            "context": {
                "subject": subject,
                "rag_context": rag_results.get("content", ""),
                "rag_citations": rag_results.get("citations", [])
            },
            "scratchpad": ""
        }

        config = {"configurable": {"thread_id": session_id}}
        
        # Execute Graph
        async for event in self.graph.astream_events(inputs, version="v1", config=config):
            kind = event["event"]
            if kind == "on_chat_model_stream":
                # Filter supervisor thinking
                node_name = event.get("metadata", {}).get("langgraph_node", "")
                if node_name == "supervisor":
                    continue
                    
                content = event["data"]["chunk"].content
                if content:
                    yield content

agent_service = AgentService()
